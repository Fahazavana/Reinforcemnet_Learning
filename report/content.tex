\newcommand{\bydef}{\overset{\scriptscriptstyle\Delta}{=}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\PP}{\mathbb{P}}

\begin{abstract}

\end{abstract}

\section{Introduction}
When it comes automatic sequential decision making with a specific objective for an agent in given environment reinforcement learning is a best approach to use. In contrast with that, given grid-world, how we can train an agent to navigate form a point A to a point B, in an optimal way with the maximum score. To solve this problem, we explore four Reinforcement Learning algorithm.


For that, we start by defining the problem in Section \ref{sec1}, and brief overview of reinforcement learning in the Section \ref{sec2}. Then we present the tabular methods and deep learning techniques respectively in section \ref{sec3} and \ref{sec4}.

Section \ref{sec5} delve on how we deal with our hyper-parameter, and model assessment. Finally in the Section \ref{sec6}, we presents our finding, compare and discuss about them.
\section{Problem statement and modelling}\label{sec1}
We aim to build an agent (red triangle) that solves an empty  $6\times6$ grid world, e.g., reaching the green cell as illustrated in Figure \ref{fig:mini-grid}. The agent (red triangle) interacts with the environment through observations, actions and rewards. We use the \texttt{mini-grid} API from the Farama foundation to simulate the environment \cite{minigrid}.
\begin{figure}
	\centering
		\begin{tikzpicture}
			\fill[darkgray!80] (-1,-1) rectangle (7,7);
			\fill[black!90] (0,0) rectangle (6,6);
			\fill[green] (5,0) rectangle (6,1);

			\foreach \x in {0, 1, 2, 3, 4, 5} {
				\draw[gray, line width=1pt] (\x,0) -- (\x,6);
				\draw[gray, line width=1pt] (0, \x) -- (6,\x);
			}

			\begin{scope}[yshift=1cm]
				\fill[green, scale=0.6, yshift=-1.6cm, xshift=-.2cm] (-1,-2) rectangle (0,-3);
				\node (t) at (0.5,-2.5) {Target};
				\fill[red, xshift=2.5cm, yshift=1.05cm] (-1,-3.8) -- (-1,-3.2) -- (-.6,-3.5) -- cycle;
				\node (t) at (2.5,-2.5) {Agent};
			\end{scope}
			\fill[red] (0.2,5.8) -- (0.2,5.2) -- (0.6,5.5) -- cycle;
		\end{tikzpicture}
	\caption{An illustration of the \texttt{MiniGrid-Empty-8x8-v0} environment}
	\label{fig:mini-grid}
\end{figure}

Depending on our configuration, the state can be a tensor or an image which is returned after each action. The agent has eight possible actions, but we only use three: turn left and right and move forward. In this setting, we deal with an episodic task, and for each episode, the agent can perform at most 256 steps.

If the agent reaches the target, it receives a reward $r$ defined by:
\begin{equation}
	r = 1 - 0.9\times \frac{\text{Number of steps}}{\text{Maximum number of steps}}
\end{equation}
otherwise, the reward is zero.


Therefore our task is to make the agent learn how to move in this grid in such a way that it maximizes its reward. For this particular environment, the agent can get at most a reward of $0.9613$ with $11$ steps. But for this work, we will consider $0.9578$ as the optimal reward corresponding to 12 steps.

\section{Reinforcement Learning technique}\label{sec2}
In this era, reinforcement learning is widely used to solve the problem of programming intelligent agents that learn how to optimise a specific task such as playing a game, or autonomous vehicle. This is done by learning a policy for sequential decision problems that maximize the discounted  cumulative future reward:
\begin{equation}
	G_t \bydef \sum_{k=t+1}^{T} \gamma^{k-t-1}R_k
\end{equation}
Where $R$ is the reward, $\gamma\in [0,1]$ is the discount factor that controls the importance fro immediate reward (near to 0) or future reward (near to 1). The agent learns the optimal policy without being explicitly told if its actions are good or bad using the reinforcement learning model depicted in Figure \ref{fig:RL}.


\begin{figure}
	\centering
	\begin{tikzpicture}
		\node[draw, rounded corners=2pt, minimum height=.75cm] (agent) at (0,0) {Agent};
		\node[draw, rounded corners=2pt, minimum height=.75cm] (env) at (0,-3) {Environment};
		\draw[->, >=latex,line width=1.5pt] (agent) -- (3, 0) -- node[right,midway, text width=1cm] {action\\$A_t$} (3, -3) -- (env);
		\draw[->, >=latex] (env.165) --node[above] {$R_{t+1}$} ++(-1, 0) node (r) {};
		\draw[->, >=latex, line width=1.5pt] (env.195) --node[below] {$S_{t+1}$} ++(-1, 0) node (s) {};
		\draw[->, >=latex] (r) -- ++(-1, 0) --node[right, midway, text width=1cm] {Reward\\$R_t$} ++(0, 2.5)-- (agent.200);
		\draw[->, >=latex,line width=1.5pt] (s)-- ++(-1.5, 0) --node[left=-0.3cm, midway, text width=1cm] {State\\$S_t$} ++(0, 3.47) -- (agent.160);
		\draw[dashed] (-2.1, -3.5)--(-2.1, -2.5);
	\end{tikzpicture}
	\caption{Reinforcement learning Model, \cite{lecture, book}}
	\label{fig:RL}
\end{figure}

Several algorithms/techniques can be used  to solve our problem with reinforcement learning, in the next section we will look:
\begin{itemize}
	\item Q-Learning
	\item SARSA (State-Action-Reward-State-Action)
	\item Deep Q-Network
	\item Deep Q-Network with RGB Image techniques.
\end{itemize}
The algorithm \ref{algo:rl_skel} show the skeleton shared by these RL algorithms.
\begin{algorithm}
	Parameters:$\ldots$\\
	\ForEach{episode}{
		(Re)Initialize the environment\\
		\ForEach{step}{
			Act and consider the observation and reward\\
			\textbf{Steps specific to each methods}\\
			\If{done or truncated}{
				Some steps for metric and monitoring\\
			}
		}
	}
	\caption{RL Algorithm Skeleton}
	\label{algo:rl_skel}
\end{algorithm}
In addition to that, we also use the $\epsilon$-greedy strategy to select the action to be performed by the agent. The agent uniformly samples an action from the possible action for the given state with a probability $\epsilon$ and uses the optimal action for the given state with a probability $1-\epsilon$. In the first case, we say that the agent is exploring, and in the second case we say that the agent is exploiting.


For this work we will start with a higher value of $\epsilon$ to allow the agent to explore the environment, then we decrease it slowly until we reach a small value, this will be controlled by :
\begin{equation}
	\epsilon_t = \epsilon_{min} + \left(\epsilon_{max} - \epsilon_{min}\right) \exp\left\{-\frac{t}{\Delta}\right\}
\end{equation}
Where $\Delta$ is the decay rate, a higher value corresponds to a slow decrease, and a small value corresponds to a fast decrease.
\section{Tabular methods: Q-Learning and SARSA}\label{sec3}
The tabular method refers to the creation of a table called Q-table, containing the value of $Q(S_t=s, A_t=a)$. This value indicates how good is acting $a$ on the state $s$.

For the two algorithms that we present in this section, the observation (a $7\times3\times3$ array) will be converted into a string to create an MD5 hash to represent the given state.
\subsection{SARSA}
The SARSA algorithm is an on-policy method, which means it iteratively refines a single policy, that also generates control action within the environments. The update rule is defined by:
\begin{equation}\label{eq:sarsa}
	Q(S_t, A_t)\leftarrow Q(S_t, A_t) + \alpha[R_{t+1} + \gamma(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]
\end{equation}
From the equation \eqref{eq:sarsa}, we obtain the Algorithm \ref{algo:sarsa} for the SARSA learning.
\begin{algorithm}
	Parameters: Step size $\alpha\in]0,1]$, small $\epsilon>0$\\
	Initialize $Q(s,a)$ for all $s\in\mathcal{S}^{+}$ and $a\in\mathcal{A}(s)$ arbitrarily, except that $Q(terminal-state, \cdot)=0$\\
	\ForEach{episode}{
		Initialize $S$\\
		Choose $A$ from $S$ using policy derived from $Q$ ($\epsilon$-greedy)\\
		\ForEach{step until $S$ is a terminal  state}{
			Take the action $A$, observe $R$, $S'$\\
			Choose $A'$ from $S'$ using policy derived from $Q$ ($\epsilon$-greedy)\\
			$Q(S, A)\leftarrow Q(S, A)$\\
			\phantom{$Q(S, A)\leftarrow$}$ + \alpha[R_{t+1} + \gamma(S', A') - Q(S_t, A_t)]$\\
			$S\leftarrow S'$\\
			$A\leftarrow A'$
		}
	}
	\caption{SARSA: on-policy learners to estimate the optimal Q-table}
	\label{algo:sarsa}
\end{algorithm}

\subsection{Q-Learning}
In contrast to SARSA, the $Q$-Learning algorithm is an off-policy algorithm, which means we optimise the target policy using different policies. The update rule is given by
\begin{equation}\label{eq:qlr}
	Q(S_t, A_t)\leftarrow Q(S_t, A_t) + \alpha[R_{t+1} + \gamma\max_a (S_{t+1}, a) - Q(S_t, A_t)]
\end{equation}
Since we have the Algorithm \ref{algo:qlr} for $Q$-learning methods.
\begin{algorithm}
	Parameters: Step size $\alpha\in(0,1]$, small $\epsilon>0$\\
	Initialize $Q(s,a)$ for all $s\in\mathcal{S}^{+}$ and $a\in\mathcal{A}(s)$ arbitrarily, except that $Q(terminal-state, \cdot)=0$\\
	\ForEach{episode}{
		Initialize $S$\\
		Choose $A$ from $S$ using $\epsilon$-greedy\\
		\ForEach{step until $S$ is a terminal  state}{
			Take the action $A$, observe $R$, $S'$\\
			Choose $A'$ from $S'$ using $\epsilon$-greedy\\
			$Q(S_t, A_t)\leftarrow Q(S_t, A_t) + \alpha[R_{t+1} + \gamma\max_a (S_{t+1}, a) - Q(S_t, A_t)]$\\
			$S\leftarrow S'$\\
		}
	}
	\caption{Q-learning algorithm to estimate the optimal $Q$-table}
	\label{algo:qlr}
\end{algorithm}



These two methods are powerful in dealing with a few states and actions, but not very efficient for numerous states and actions. We can replace the table with a function approximator as we will see in the next section.



\section{Function approximation: Deep $Q$-network}\label{sec4}
The idea of function approximation is to replace the value function with its approximation instead of using a look-up table. The function can take
\begin{itemize}
	\item the state and action as input, and $q(s,a)$ as output,
	\item or the state as input and $q(s,a)$ as output.
\end{itemize}
This is illustrated in the figure \ref{fig:fct-approx}.
\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{scope}
			\node (s) at (-0.5,-1.25) {$s$};
			\node (a) at (0.5,-1.25) {$a$};
			\node (q) at (0,1.25) {$\hat{q}(s, a, \mbf{w})$};
			\node[draw, rounded corners=2pt, line width=1pt, minimum width=2cm, minimum height=1cm] (fct) at (0,0) {$\mbf{w}$};
			\draw[->, >=latex,line width=1pt, line cap=round] (s) --++(0,0.75);
			\draw[->, >=latex,line width=1pt, line cap=round] (a) --++(0,0.75);
			\draw[->, >=latex,line width=1pt, line cap=round] (fct) -- (q);
		\end{scope}
		\begin{scope}[xshift=4cm]
			\node (s) at (0,-1.25) {$s$};
			\node (q1) at (-1.25,1.25) {$\hat{q}(s, a_1, \mbf{w})$};
			\node (q2) at (0,1.25) {$\cdots$};
			\node (q3) at (1.25,1.25) {$\hat{q}(s, a_m, \mbf{w})$};
			\node[draw, rounded corners=2pt, line width=1pt, minimum width=2.75cm, minimum height=1cm] (fct) at (0,0) {$\mbf{w}$};
			\draw[->, >=latex,line width=1pt, line cap=round] (s) --++(0,0.75);
			\draw[<-, >=latex,line width=1pt, line cap=round] (q1) --++ (0, -.75);
			\draw[<-, >=latex,line width=1pt, line cap=round] (q3) --++ (0, -.75);
		\end{scope}
	\end{tikzpicture}
	\caption{Illustration of function approximation}
	\label{fig:fct-approx}
\end{figure}


In our case, we will use a neural network as a function approximation. Thus we will consider two types.

A feed-forward neural network (MLP) which takes a vector $\mbf{u}\in\RR^{49\times1}$ (state) as input and $q(s,a_1), q(s,a_2),q(s,a_3)$ as output.
\begin{table}
	\centering
	\begin{adjustbox}{max width=\linewidth}
		\begin{tabular}{@{}llrr@{}}
			\toprule
			\textbf{Layer} & \textbf{Input} & \textbf{Output} & \textbf{\# Param} \\
			\midrule
			\textbf{DQN}                     & {[1, 49]}           & {[1, 3]}              & --                           \\
			\quad \textbf{Sequential: 1-1}   & {[1, 49]}           & {[1, 3]}              & --                           \\
			\quad \quad Linear: 2-1          & {[1, 49]}           & {[1, 64]}             & 3,200                        \\
			\quad \quad ReLU: 2-2            & {[1, 64]}           & {[1, 64]}             & --                           \\
			\quad \quad Linear: 2-3          & {[1, 64]}           & {[1, 32]}             & 2,080                        \\
			\quad \quad ReLU: 2-4            & {[1, 32]}           & {[1, 32]}             & --                           \\
			\quad \quad Linear: 2-5          & {[1, 32]}           & {[1, 3]}              & 99                           \\
			\midrule
			\textbf{Total params:}           &                     &                      & \textbf{5,379}                 \\
			\textbf{Trainable params:}       &                     &                      & \textbf{5,379}                 \\
			\textbf{Non-trainable params:}   &                     &                      & \textbf{0}                     \\
			\textbf{Total mult-adds (M):}    &                     &                      & \textbf{0.01}                  \\
			\midrule
			\textbf{Input size (MB):}        &                     &                      & \textbf{0.00}                  \\
			\textbf{Forward/backward pass size (MB):} &            &                      & \textbf{0.00}                  \\
			\textbf{Params size (MB):}       &                     &                      & \textbf{0.02}                  \\
			\textbf{Estimated Total Size (MB):} &                   &                      & \textbf{0.02}                  \\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption{Model Summary for DQN}
	\label{tab:dqn}
\end{table}
Similarly, Convolutional Neural networks (CNN), take a stack of four frames (images) at four successive time steps, e.g. a $4\times56\times56$ tensor as input and the same output as the MLP, The architecture of these neural networks are described in respectively in \ref{tab:dqn} and \ref{tab:cnn-dqn}.

\begin{table}
	\centering
	\begin{adjustbox}{max width=\linewidth}
	\begin{tabular}{@{}llcrr@{}}
		\toprule
		Layer (type:depth-idx) & Input& Kernel& Output & \# Param \\
		\midrule
		\textbf{CNN\_DQN}                                   & [1, 4, 56, 56]   & --            & [1, 3]           & --     \\
		\quad \textbf{Sequential: 1-1}                   & [1, 4, 56, 56]   & --            & [1, 512]         & --     \\
		\quad \quad Conv2d: 2-1                  & [1, 4, 56, 56]   & [3, 3]        & [1, 16, 27, 27]  & 576    \\
		\quad \quad BatchNorm2d: 2-2             & [1, 16, 27, 27]  & --            & [1, 16, 27, 27]  & 32     \\
		\quad \quad ReLU: 2-3                    & [1, 16, 27, 27]  & --            & [1, 16, 27, 27]  & --     \\
		\quad \quad Conv2d: 2-4                  & [1, 16, 27, 27]  & [3, 3]        & [1, 32, 13, 13]  & 4,608  \\
		\quad \quad BatchNorm2d: 2-5             & [1, 32, 13, 13]  & --            & [1, 32, 13, 13]  & 64     \\
		\quad \quad ReLU: 2-6                    & [1, 32, 13, 13]  & --            & [1, 32, 13, 13]  & --     \\
		\quad \quad Conv2d: 2-7                  & [1, 32, 13, 13]  & [3, 3]        & [1, 64, 6, 6]    & 18,432 \\
		\quad \quad BatchNorm2d: 2-8             & [1, 64, 6, 6]    & --            & [1, 64, 6, 6]    & 128    \\
		\quad \quad ReLU: 2-9                    & [1, 64, 6, 6]    & --            & [1, 64, 6, 6]    & --     \\
		\quad \quad Conv2d: 2-10                 & [1, 64, 6, 6]    & [3, 3]        & [1, 128, 2, 2]   & 73,728 \\
		\quad \quad BatchNorm2d: 2-11            & [1, 128, 2, 2]   & --            & [1, 128, 2, 2]   & 256    \\
		\quad \quad Flatten: 2-12                & [1, 128, 2, 2]   & --            & [1, 512]         & --     \\
		\quad \textbf{Sequential: 1-2}                   & [1, 512]         & --            & [1, 3]           & --     \\
		\quad \quad Linear: 2-13                 & [1, 512]         & --            & [1, 64]          & 32,832 \\
		\quad \quad ReLU: 2-14                   & [1, 64]          & --            & [1, 64]          & --     \\
		\quad \quad Linear: 2-15                 & [1, 64]          & --            & [1, 3]           & 195    \\
		\midrule
		\textbf{Total params}                              &                   &              &                   & \textbf{130,851}\\
		\textbf{Trainable params:}                         &                   &              &                   & \textbf{130,851}\\
		\textbf{Non-trainable params:}                     &                   &              &                   & 0      \\
		\textbf{Total mult-adds (M):}                      &                   &              &                   & \textbf{2.19}   \\
		\midrule
		\textbf{Input size (MB):}                          &                   &              &                   & \textbf{0.05}    \\
		\textbf{Forward/backward pass size (MB):}          &                   &              &                   & \textbf{0.32}    \\
		\textbf{Params size (MB):}                         &                   &              &                   & \textbf{0.52}     \\
		\textbf{Estimated Total Size (MB):}                &                   &              &                   & \textbf{0.89}     \\
		\bottomrule
	\end{tabular}
	\end{adjustbox}
	\caption{Model Architecture for DQN-Image}
	\label{tab:cnn-dqn}
\end{table}




As we do not have any labels to train the networks, we use two networks: the \texttt{policy\_net} and \texttt{target\_net}. The first one is optimised with the \texttt{Adam} optimizer, while the second one is fixed, and used to generate a sort of label for the \texttt{policy\_net}. We also synchronize the weight of the two networks in a regular period of the training, to move toward the optimal values.


To train these networks, we need a dequeue (double end-queue) that stores a given number of the experience of the agent formed by the current state, action, next state, and reward, we call this a replay memory $\mathcal{D}$. As new experiences are added the oldest data is pushed out of the dequeue.
Once we reach have batch size (enough number) of experience, we start to sample from $\mathcal{D}$ and optimize the \text{policy\_net} parameter by minimizing the Mean Squared Error (MSE) given by:
\begin{equation}
	\mathcal{L} = \frac{1}{N}\sum{i=1}^{N}\left[R_i + \gamma\max_{a'}\hat{Q}(s_i', a', \mbf{w}^{-}) - \hat{Q}(s_i, a, \mbf{w})\right]^2
\end{equation}
where $R_i + \gamma\max_{a'}\hat{Q}(s_i', a', \mbf{w}^{-})$ is computed using \texttt{target\_net}, and $\hat{Q}(s_i', a', \mbf{w}^{-})$ is $0$ if $s_i'$ is terminal state, while $\hat{Q}(s_i, a, \mbf{w})$ is computed with \texttt{policy\_net}.


Note that, the input of these networks (observation) is normalized by rescaling their value between $[-1,1]$. In particular, we convert the RGB image into a grey-scale image for the CNN architecture.
We can summarize the whole process in the algorithm \ref{algo:dqn}.
\begin{algorithm}
	Initialize \texttt{policy\_net} and \texttt{target\_net}\\
	Initialize the environments\\
	Set the decay rate for the epsilon decreasing\\
	Set the updating period of \texttt{target\_net}\\
	Set the total step to $0$\\
	Create a replay memory $\mathcal{D}$\\
	\ForEach{episode}{
		Set step to 0\\
		Make a new episode\\
		Observe the first state\\
		\While{not( done or truncated)}{
			Choose $A$ from $S$ using policy derived from $Q$ ($\epsilon$-greedy)\\
			Increment the total training step\\
			Execute $A$, observe $R$ and the new state $S'$\\
			Store Transition $<S, A, S', R>$ in $\mathcal{D}$\\
			Compute the $\mathcal{L}$ and do a gradient descent step\\
			\If{updating period}{
				Copy the \texttt{policy\_net} parameter to \texttt{target\_net}
			}
		}
	}
	\caption{Training a DQN to estimate the optimal policy}
	\label{algo:dqn}
\end{algorithm}
\section{Hyper-parameters tuning and Evaluation}\label{sec5}
\subsection{Hyper-parameters}
like all machine learning problems we have several parameters to tune to get the optimal results. Grid search is one of the best methods, it iterates through all the possible combinations of predefined parameters set. The drawback of this method is the time complexity which can explode considerably when we hyper-parameter space is big and the algorithm is quite slow.

So, instead of using that approach, we will use trial and error to find good hyper-parameters such as $\gamma$, $\alpha$, and the number of episodes. To do that we start we some values, and we adjust these parameters according to the obtained results.

\subsection{Model evaluation}
To evaluate the model, we run the agents in the environment for $1000$ episodes, then compute the completion rate, the reward average, and the averaged steps for each method.

We also investigate these values for the training process to assess the speed and efficiency of each method, in addition to the plot of the loss and accumulated rewards.
\section{Results and discussion}\label{sec6}
In this section we discus about our results an present them. We smoothed some of the curve using the exponential moving average wit a factor of $0.7$, to obtain a better visualisation.
\subsection{Q-Learning}
After training the $Q$-Learning algorithm for $512$ episodes, using a discount factor $\gamma=0.9$, $\epsilon$ starting from $1$ to $0.01$ with decay rate of $\Delta=3000$ and a learning rate $\alpha=0.1$. We observe in Figure \ref{fig:qlog} the elution of the averaged cumulated squared TD-error and, reward, and steps.
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{figures/QLearning_episode.pdf}
	\caption{Q-Learning per episode training metrics}
	\label{fig:qlog}
\end{figure}
During the phase were we $\epsilon$ is big, the agent explore its environment. On it reach some point, the agent start to exploit the optimal policy found by algorithm. We resume the training and an evaluation phase in Table \ref{tab:ql}, and the optimal policy is shown in figure \ref{fig:optimal}.
\begin{table}
	\centering
	\begin{tabular}{@{}lrr@{}}
		\cmidrule(l){2-3}
		& Train & Evaluation \\ \midrule
		Episodes        &  512      &  1000          \\
		Completion rate &   92.97\%    &  100\%           \\
		Average rewards &   0.808    &    0.958        \\
		Average steps   &    53   &     12       \\ \bottomrule
	\end{tabular}
	\caption{Training and evaluation summary for {$Q$-Learning}}
	\label{tab:ql}
\end{table}

\subsection{SARSA-Learning}
Similarly, we train the $SARSA$-Learning algorithm for $600$ episodes, using a discount factor $\gamma=0.9$, $\epsilon$ starting from $1$ to $0.01$ with decay rate of $\Delta=3000$ and a learning rate $\alpha=0.1$. We observe in Figure \ref{fig:sarsalog} the elution of the averaged cumulated squared TD-error and, reward, and steps.
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{figures/SARSALearning_episode.pdf}
	\caption{SARSA-Learning per episode training metrics}
	\label{fig:sarsalog}
\end{figure}
We observe some similar pattern in the learning curve of the $Q$-Learning and SARSA. But SARSA required further episode before being stable.  We resume the training and an evaluation phase in Table \ref{tab:sarsa}, and the optimal policy is shown in figure \ref{fig:optimal}.
\begin{table}
	\centering
	\begin{tabular}{@{}lrr@{}}
		\cmidrule(l){2-3}
		& Train & Evaluation \\ \midrule
		Episodes        &  600      &  1000          \\
		Completion rate &   95.67\%    &  100\%           \\
		Average rewards &   0.844    &    0.958        \\
		Average steps   &    43   &     12       \\ \bottomrule
	\end{tabular}
		\caption{Training and evaluation summary for {SARSA-Learning}}
	\label{tab:sarsa}
\end{table}

Overall the two $Q$-Learning and SARSA, performed well and give similar result, and yielded the same policy. However, as we can observe in Figure \ref{fig:optimal}, several path with 12 steps is possible, and the choice of the agent seems to be the middle (nearest) one. It some how he summarize these path, but it may also because of the randomness introduced by the $\epsilon$-greedy strategy.




Now the two next subsection presents our finding using deep learning technique.
\subsection{Deep Q-Network Learning}
We trained our  Deep $Q$-Network for $2000$ episodes, using a discount factor $\gamma=0.9$, $\epsilon$ starting from $1$ to $0.01$ with decay rate of $\Delta=10^4$ and a learning rate $\alpha=10^{-4}$. In addition, we use a replay memory of size $4096$, and a batch $256$. We synchronize the \texttt{policy\_net} and \texttt{target\_net} every $2048$ steps of the agent. The evolution of the training process can be observed in Figure \ref{fig:dqnlog}, it shows the averaged accumulated squared TD-error (MSE Loss) and, reward, and steps.
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{figures/DQNLearning_episode.pdf}
	\caption{DQN-Learning per episode training metrics}
	\label{fig:dqnlog}
\end{figure}
During the phase were we $\epsilon$ the network give some random value until it starts to find the right policy that he refine. Once $\epsilon$ is small enough, the agent start to exploit and refine the optimal policy found by algorithm. We resume the training and an evaluation phase in Table \ref{tab:summ_dqn}, and the optimal policy is shown in figure \ref{fig:optimal}.
\begin{table}
	\centering
	\begin{tabular}{@{}lrr@{}}
		\cmidrule(l){2-3}
		& Train & Evaluation \\ \midrule
		Episodes        &  2000      &  1000          \\
		Completion rate &   99.2\%    &  100\%           \\
		Average rewards &   0.912    &    0.958        \\
		Average steps   &    25   &     12       \\ \bottomrule
	\end{tabular}
	\caption{Training and evaluation summary for DQN}
	\label{tab:summ_dqn}
\end{table}


Now, instead of feeding a vector, we use image as input and we report the results in the next section.
\subsection{Deep Q-Network Learning with RGB Image}
For this approach, we train the Deep Q-convolutional networks for $1500$ episodes, using a discount factor $\gamma=0.9$, $\epsilon$ starting from $1$ to $0.01$ with decay rate of $\Delta=10^4$ and a learning rate $\alpha=10^{-4}$. In addition, we use a replay memory of size $4096$, and a batch $128$. We synchronize the \texttt{policy\_net} and \texttt{target\_net} every $2048$ steps of the agent. The evolution of the training process can be observed in Figure \ref{fig:dqn-img}, it shows the averaged accumulated squared TD-error (MSE Loss) and, reward, and steps.
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{figures/DQNRGBLearning_episode.pdf}
	\caption{DQN-Learning with RGB Image technique per episode training metrics}
	\label{fig:dqn-img}
\end{figure}
At the begging of the one episodes was slow. Then the duration of one epoch started slowly to decrease and become faster. This is due to the fact that the network/agent does not yet have any idea about the environment so it explore, (reson why we started with a higher value of $\epsilon$). We resume the training and an evaluation phase in Table \ref{tab:summ_dqn}, and the optimal policy is shown in figure \ref{fig:optimal}.
\begin{table}
	\centering
	\begin{tabular}{@{}lrr@{}}
		\cmidrule(l){2-3}
		& Train & Evaluation \\ \midrule
		Episodes        &  1500      &  1000          \\
		Completion rate &   98\%    &  100\%           \\
		Average rewards &   0.892    &    0.961        \\
		Average steps   &    30   &     11       \\ \bottomrule
	\end{tabular}
	\caption{Training and evaluation summary for DQN-Image}
	\label{tab:summ-dqn-img}
\end{table}

The two deep learning approach resulted in good performance, especially the DQ-CNN with 11 steps against 12 for the simple DQN.
However, in terms of speed the simple DQN is faster, and may be with an efficient hyperameters tuning like grid search, we will possibly get same results for both methods.

\begin{figure}
	\centering
	\begin{tikzpicture}
	\fill[darkgray!80] (-1,-1) rectangle (7,7);
	\fill[black!90] (0,0) rectangle (6,6);
	\fill[green] (5,0) rectangle (6,1);

	\foreach \x in {0, 1, 2, 3, 4, 5} {
		\draw[gray, line width=1pt] (\x,0) -- (\x,6);
		\draw[gray, line width=1pt] (0, \x) -- (6,\x);
	}
	\draw[dashed, line width=1pt, color=cyan, line cap=round, line width=2pt, rounded corners] (0.5,5.4)--++(1.9,0)--++(0,-5.2)--++(3.1,0);
	\draw[dashed, line width=1pt, color=orange, line cap=round, line width=2pt, rounded corners] (0.5,5.5)--++(2,0)--++(0,-5.2)--++(3,0);
	\draw[dashed, line width=1pt, color=blue, line cap=round, line width=2pt, rounded corners] (0.5,5.6)--++(3,0)--++(0,-5.2)--++(2,0);
	\draw[dashed, line width=1pt, color=yellow, line cap=round, line width=2pt, rounded corners] (0.5,5.7)--++(5,0)--++(0,-5.2);

	\begin{scope}[yshift=1cm]
			\fill[green, scale=0.6, yshift=-1.6cm, xshift=-.2cm] (-1,-2) rectangle (0,-3);
			\node (t) at (0.5,-2.5) {Target};
			\fill[red, xshift=0.35cm] (-1,-3.8) -- (-1,-3.2) -- (-.6,-3.5) -- cycle;
			\node (t) at (0.5,-3.5) {Agent};
			\draw[color=cyan, line width=2pt, line cap=round,dashed] (1.5,-2.5) --++(0.5,0) node[right, black] {Q-Learning};
			\draw[color=orange, line width=2pt, line cap=round,dashed] (1.5,-3.5) --++(0.5,0) node[right, black] {SARSA};
			\draw[color=blue, line width=2pt, line cap=round,dashed] (4.5,-2.5) --++(0.5,0) node[right, black] {DQN};
			\draw[color=yellow, line width=2pt, line cap=round,dashed] (4.5,-3.5) --++(0.5,0) node[right, black] {DQN-RGB};
	\end{scope}
	\fill[red] (0.2,5.8) -- (0.2,5.2) -- (0.6,5.5) -- cycle;
\end{tikzpicture}
\caption{Optimal Policy found by each algorithm}
\label{fig:optimal}
\end{figure}
\section{Conclusion}
In conclusion, we have explored four Reinforcement Learning algorithm. Q-Learning and SARSA for Tabular methods in one side and DQN and DQN-CNN for Deep leaning technique for reinforcement Learning in the other side. Each algorithm achieved 100\% completion rate, with an average of 12 steps over 1000 episode for Q-Learning, SARSA, DQN, while the DQN-CNN did 11 steps, even it was a bit slow.


\begin{thebibliography}{1}
\bibitem{lecture} Prof Herman Engelbrecht, Reinforcement Learning Lecture, Stellenbosch University, 2024.
\bibitem{book}  Richard S. Sutton, Andrew Barto, Reinforcement Learning: An Introduction.
\bibitem{minigrid} Farama Foundation, \texttt{minigrid} \href{https://minigrid.farama.org/}{https://minigrid.farama.org/}
\end{thebibliography}